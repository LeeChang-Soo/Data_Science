{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "bf55b1ccd93d455b96ef4935ae5ce8f7"
   },
   "source": [
    "# 확률론적 언어 모형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "eca6a27f0cfb456f9c6739bba770cc77"
   },
   "source": [
    "확률론적 언어 모형(Probabilistic Language Model)은 $m$개의 단어 $w_1, w_2, \\ldots, w_m$ 열(word sequence)이 주어졌을 때 문장으로써 성립될 확률 $P(w_1, w_2, \\ldots, w_m)$ 을 출력함으로써 이 단어 열이 실제로 현실에서 사용될 수 있는 문장(sentence)인지를 판별하는 모형이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "e29531311ac34e6eba0d4227fc3eb14b"
   },
   "source": [
    "이 확률은 각 단어의 확률과 단어들의 조건부 확률을 이용하여 다음과 같이 계산할 수 있다.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "P(w_1, w_2, \\ldots, w_m) &=& P(w_1, w_2, \\ldots, w_{m-1}) \\cdot P(w_m\\;|\\; w_1, w_2, \\ldots, w_{m-1}) \\\\\n",
    "&=& P(w_1, w_2, \\ldots, w_{m-2}) \\cdot P(w_{m-1}\\;|\\; w_1, w_2, \\ldots, w_{m-2}) \\cdot P(w_m\\;|\\; w_1, w_2, \\ldots, w_{m-1}) \\\\\n",
    "&=& P(w_1) \\cdot P(w_2 \\;|\\; w_1) \\cdot  P(w_3 \\;|\\; w_1, w_2) P(w_4 \\;|\\; w_1, w_2, w_3) \\cdots P(w_m\\;|\\; w_1, w_2, \\ldots, w_{m-1})\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "여기에서 $P(w_m\\;|\\; w_1, w_2, \\ldots, w_{m-1})$ 은 지금까지 $w_1, w_2, \\ldots, w_{m-1}$라는 단어 열이 나왔을 때, 그 다음 단어로 $w_m$이 나올 조건부 확률을 말한다. 여기에서 지금까지 나온 단어를 **문맥(context)** 정보라고 한다.\n",
    "\n",
    "이 때 조건부 확률을 어떻게 모형화하는냐에 따라 \n",
    "* 유니그램 모형 (Unigram Model)\n",
    "* 바이그램 모형 (Bigram Model)\n",
    "* N-그램 모형 (N-gram Model)\n",
    "\n",
    "등으로 나뉘어 진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "b2114670ba754f5f98b95b58fa4a02d3"
   },
   "source": [
    "## 유니그램 모형 (Unigram  Model)\n",
    "\n",
    "만약 모든 단어의 활용이 완전히 서로 독립이라면 단어 열의 확률은 다음과 같이 각 단어의 확률의 곱이 된다. 이러한 모형을 유니그램 모형 (Unigram  Model)이라고 한다.\n",
    "\n",
    "$$ P(w_1, w_2, \\ldots, w_m) = \\prod_{i=1}^m P(w_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "657f733b6e924e30a8a64575182f480b"
   },
   "source": [
    "## 바이그램 모형 (Bigram Model)\n",
    "\n",
    "만약 단어의 활용이 바로 전 단어에만 의존한다면 단어 열의 확률은 다음과 같다. 이러한 모형을 Bigram 모형 또는 마코프 모형(Markov Model)이라고 한다.\n",
    "\n",
    "$$ P(w_1, w_2, \\ldots, w_m) = P(w_1) \\prod_{i=2}^{m} P(w_{i}\\;|\\; w_{i-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "5523c981bdf64e78ad2438a51f53bf4d"
   },
   "source": [
    "## N-그램 모형 (N-gram Model)\n",
    "\n",
    "만약 단어의 활용이 바로 전 $n$개의 단어에만 의존한다면 단어 열의 확률은 다음과 같다. 이러한 모형을 N-gram 모형이라고 한다.\n",
    "\n",
    "$$ P(w_1, w_2, \\ldots, w_m) = P(w_1) \\prod_{i=n}^{m} P(w_{i}\\;|\\; w_{i-1}, \\ldots, w_{i-n}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "2ebfcf9cc8334977a10227880b4fe044"
   },
   "source": [
    "## 확률 추정 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "f8d74e7acdf74dd2b61edc394f1d378c"
   },
   "source": [
    "실제 텍스트 코퍼스(corpus)에서 확률을 추정하는 방법은 다음과 같다. 여기에서는 바이그램의 경우를 살펴본다.\n",
    "\n",
    "일단 모든 문장에 문장의 시작과 끝을 나타내는 특별 토큰을 추가한다. 예를 들어 문장의 시작은 `SS`, 문장의 끝은 `SE` 이라는 토큰을 사용할 수 있다.  \n",
    "\n",
    "바이그램 모형에서는 전체 문장의 확률은 다음과 같이 조건부 확률의 곱으로 나타난다.\n",
    "\n",
    "$$ P(\\text{SS I am a boy SE}) = P(\\text{I}\\;|\\; \\text{SS}) \\cdot P(\\text{am}\\;|\\; \\text{I}) \\cdot P(\\text{a}\\;|\\; \\text{am}) \\cdot P(\\text{boy}\\;|\\; \\text{a}) \\cdot P(\\text{SE}\\;|\\; \\text{boy}) $$\n",
    "\n",
    "조건부 확률은 다음과 같이 추정한다.\n",
    "\n",
    "$$ P(w_{i}\\;|\\; w_{i-1}) = \\dfrac{C(w_{i}, w_{i-1})}{C(w_{i-1})} $$\n",
    "\n",
    "위 식에서 $C(w_{i}, w_{i-1})$은 전체 코퍼스에서 $(w_{i}, w_{i-1})$라는 바이그램이 나타나는 횟수이고 $C(w_{i-1})$은 전체 코퍼스에서 $(w_{i-1})$라는 유니그램(단어)이 나타나는 횟수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "e2cedcc1da814a668365dbc71f2723b8"
   },
   "source": [
    "## 바이그램 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "cdc1ac402c1347aabb68e02d48863c15"
   },
   "source": [
    "다음은 nltk 패키지의 샘플 코퍼스인 movie_reviews의 텍스트를 기반으로 바이그램 모형을 추정하고 모형 확률로부터 랜덤하게 문장을 생성하는 예제이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "41f42fc50f544f358d3440b47a7788b8"
   },
   "source": [
    "우선 다음과 같이 문장(단어 리스트)의 리스트를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "school_cell_uuid": "57a444735891416c9bab0a3fd60e1a4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/dockeruser/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/dockeruser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "school_cell_uuid": "6e78bdf4121b4a2eb33abee2f3615cdd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "sentences = []\n",
    "for s in movie_reviews.sents():\n",
    "    s.insert(0, \"SS\")\n",
    "    s.append(\"SE\")\n",
    "    if len(s) > 4:\n",
    "        sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "school_cell_uuid": "ac1e230af8ac4e98b499fe97ac58c74d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SS', 'they', 'get', 'into', 'an', 'accident', '.', 'SE']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "2c8faad6f13d482ba8c9e3ad21299f42"
   },
   "source": [
    "이제 이 텍스트 정보로부터 확률값을 추정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "school_cell_uuid": "aa667739854f437db252c5699e3b70d4"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def calculate_bigram(sentences):\n",
    "    bigram = {}\n",
    "    for s in sentences:\n",
    "        context = \"SS\"\n",
    "        for i, w in enumerate(s[1:]):\n",
    "            if context not in bigram:\n",
    "                bigram[context] = Counter()\n",
    "            if bigram[context][w] == 0:\n",
    "                bigram[context][w] = 1\n",
    "            bigram[context][w] += 1\n",
    "            context = w\n",
    "    for context in bigram.keys():\n",
    "        total = sum(bigram[context].values())\n",
    "        for w in bigram[context]:\n",
    "            bigram[context][w] /= total\n",
    "    return bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "school_cell_uuid": "e3351047f3e541fca2bbd78bd7447746"
   },
   "outputs": [],
   "source": [
    "bigram = calculate_bigram(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "674312eb5fe2447dbf27b16fdaca8ec6"
   },
   "source": [
    "* 문장의 처음에 올 수 있는 단어들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "school_cell_uuid": "3ceef9e0eb8d4e22adae34cc1cc1c2d9",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.11231263830320237),\n",
       " ('it', 0.043575076893101194),\n",
       " ('i', 0.03379121261464379),\n",
       " ('but', 0.02523207103391647),\n",
       " ('and', 0.024160438673402642),\n",
       " ('he', 0.023269731256871668),\n",
       " ('in', 0.023102723616272112),\n",
       " ('this', 0.022963550582439148),\n",
       " ('there', 0.0180507424881355),\n",
       " ('as', 0.013249272820898222)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[\"SS\"].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "166b898dd6024747b2f70e50a6eb8d44"
   },
   "source": [
    "* we 다음에 올 수 있는 단어들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "school_cell_uuid": "fb7343130a3c429c8923243273a6b47d",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'\", 0.12985751295336787),\n",
       " ('are', 0.07674870466321243),\n",
       " ('see', 0.059261658031088085),\n",
       " ('get', 0.052461139896373056),\n",
       " ('have', 0.05116580310880829),\n",
       " ('can', 0.0391839378238342),\n",
       " ('don', 0.03756476683937824),\n",
       " ('know', 0.03432642487046632),\n",
       " ('never', 0.01878238341968912),\n",
       " ('learn', 0.018458549222797927)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[\"we\"].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "3bb3ec30ac8a485aa9dbed68db72b460"
   },
   "source": [
    "### 컨텍스트-단어 조합의 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "b57264a7f65f4b03ae3fe3791844863b"
   },
   "source": [
    "트레이닝이 끝나면 조건부 확률의 값을 보거나 샘플 문장을 입력해서 문장의 로그 확률을 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "43e11fdddeae472fbbe2fdcaab43d5c1"
   },
   "source": [
    "\"i\" 라는 단어가 나온 뒤에 \"was\"이라는 단어가 나올 확률을 계산하면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "school_cell_uuid": "64d572869495495c9a17e3a0ace12fca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.053622421998942356"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[\"i\"][\"was\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "school_cell_uuid": "390b48ee6f32465b925fb31675399323"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017556848228450557"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[\"i\"][\"am\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "school_cell_uuid": "4acb0e6910d84ee9b4e1f9c01b1a43ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00031729243786356425"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[\"i\"][\"is\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "school_cell_uuid": "94a29783e59f47d485b999faa441acc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00021152829190904283"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[\"i\"][\"are\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "school_cell_uuid": "da6d24d6bc704472be9137600671c6dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9612387969875893"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[\".\"][\"SE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "school_cell_uuid": "80be41eeb2944c7786fc213e2e59a2d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'SE': 0.9612387969875893,\n",
       "         \"'\": 0.0010735373054213634,\n",
       "         '\"': 0.02922949299760894,\n",
       "         ')': 0.00821418695814831,\n",
       "         \"''\": 6.506286699523415e-05,\n",
       "         ']': 0.0001789228842368939})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[\".\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "e361fed8dd5b400dbe275b70c4183f17"
   },
   "source": [
    "### 문장의 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "school_cell_uuid": "90e10f1655004a15aae5bbba2e20c21f"
   },
   "outputs": [],
   "source": [
    "def sentence_score(s):\n",
    "    p = 0.0\n",
    "    for i in range(len(s) - 1):\n",
    "        c = s[i]\n",
    "        w = s[i + 1]\n",
    "        p += np.log(bigram[c][w] + np.finfo(float).eps)\n",
    "    return np.exp(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "school_cell_uuid": "48ba48c46a8d49468846a87a324165d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.288036438066686e-08"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = [\"i\", \"am\", \"a\", \"boy\", \".\"]\n",
    "sentence_score(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "school_cell_uuid": "eee8a42e80a44e0e86f749766da11561"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9683389110380156e-38"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = [\"i\", \"is\", \"boy\", \"a\", \".\"]\n",
    "sentence_score(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "94439039596542ccb0dfc71abb35cd99"
   },
   "source": [
    "### 문장의 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "e2391827dcd348cd9b4ef80aa457a3b8"
   },
   "source": [
    "이 모형을 기반으로 임의의 랜덤한 문장을 생성할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "school_cell_uuid": "060a067a4b5d407785a452fe24ec01a1"
   },
   "outputs": [],
   "source": [
    "def generate_sentence(seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    c = \"SS\"\n",
    "    sentence = []\n",
    "    while True:\n",
    "        if c not in bigram:\n",
    "            break\n",
    "        words, probs = zip(*[(k, v) for k, v in bigram[c].items()])\n",
    "        idx = np.argmax(np.random.multinomial(1, probs, (1,)))\n",
    "        w = words[idx]\n",
    "        \n",
    "        if w == \"SE\":\n",
    "            break\n",
    "        elif w in [\"i\", \"ii\", \"iii\"]:\n",
    "            w2 = w.upper()\n",
    "        elif w in [\"mr\", \"luc\", \"i\", \"robin\", \"williams\", \"cindy\", \"crawford\"]:\n",
    "            w2 = w.title()\n",
    "        else:\n",
    "            w2 = w\n",
    "        \n",
    "        if c == \"SS\":\n",
    "            sentence.append(w2.title())\n",
    "        elif c in [\"`\", \"\\\"\", \"'\", \"(\"]:\n",
    "            sentence.append(w2)\n",
    "        elif w in [\"'\", \".\", \",\", \")\", \":\", \";\", \"?\"]:\n",
    "            sentence.append(w2)\n",
    "        else:\n",
    "            sentence.append(\" \" + w2)\n",
    "            \n",
    "        c = w\n",
    "    return \"\".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "school_cell_uuid": "2b890a85b23549319b78030d60c6ca43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alexandre dumas may suspect he at being can be honest here goes awol, but he trusts affleck - see this documentary.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(82)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "9724b0639ce0426982305478040fed0a"
   },
   "source": [
    "이번에는 한글 자료를 이용해보자 코퍼스로는 아래의 웹사이트에 공개된 Naver sentiment movie corpus 자료를 사용한다.\n",
    "* https://github.com/e9t/nsmc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "school_cell_uuid": "a9bce2d286394d06a63b89dfd222341f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 20 ms, total: 20 ms\n",
      "Wall time: 470 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!wget -nc -q https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "school_cell_uuid": "ecbca2ea7add46689a20850921ba48d8"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open(\"ratings_train.txt\", encoding='utf-8') as f:\n",
    "    data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "    data = data[1:]   # header 제외\n",
    "    \n",
    "docs = [row[1] for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "school_cell_uuid": "d159e76a59cc459b91c4729040875096"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "tagger = Twitter()\n",
    "\n",
    "def tokenize(doc):\n",
    "    return [\"SS\"] + ['/'.join(t) for t in tagger.pos(doc, norm=True, stem=True)] + [\"SE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "school_cell_uuid": "06bc1e03b2224525b197bebf77efeb08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 27s, sys: 1.52 s, total: 3min 28s\n",
      "Wall time: 3min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentences = [tokenize(d) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "school_cell_uuid": "bfbe12e798714434a81b03324e5c9300"
   },
   "outputs": [],
   "source": [
    "bigram = calculate_bigram(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "school_cell_uuid": "9de86e17eb794d0f806a9aec2fe84412"
   },
   "outputs": [],
   "source": [
    "def korean_most_common(c, n, pos=None):\n",
    "    if pos is None:\n",
    "        return bigram[tokenize(c)[0]].most_common(n)\n",
    "    else:\n",
    "        return bigram[\"/\".join([c, pos])].most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "school_cell_uuid": "ed0182db3a1543118826589a7fad9ae6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('이/Determiner', 0.01753546464438441),\n",
       " ('정말/Noun', 0.0164754624638085),\n",
       " ('이/Noun', 0.015282202866245896),\n",
       " ('진짜/Noun', 0.01355591360073655),\n",
       " ('영화/Noun', 0.01299259815620192),\n",
       " ('재밌다/Adjective', 0.011248137424739846),\n",
       " ('아/Exclamation', 0.011193623026881655),\n",
       " ('너무/Noun', 0.010363792748373653),\n",
       " ('평점/Noun', 0.00959453402304142),\n",
       " ('내/Noun', 0.009328019189068046)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_most_common(\"나\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "school_cell_uuid": "219d4ff3eabe4d1e82481d45633f73ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('이/Determiner', 0.01753546464438441),\n",
       " ('정말/Noun', 0.0164754624638085),\n",
       " ('이/Noun', 0.015282202866245896),\n",
       " ('진짜/Noun', 0.01355591360073655),\n",
       " ('영화/Noun', 0.01299259815620192),\n",
       " ('재밌다/Adjective', 0.011248137424739846),\n",
       " ('아/Exclamation', 0.011193623026881655),\n",
       " ('너무/Noun', 0.010363792748373653),\n",
       " ('평점/Noun', 0.00959453402304142),\n",
       " ('내/Noun', 0.009328019189068046)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_most_common(\"의\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "school_cell_uuid": "88b010aa6dcb4fba878304beca9ef7e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SE', 0.34900406798404404),\n",
       " ('영화/Noun', 0.009070682868389525),\n",
       " ('이/Noun', 0.007806843165391856),\n",
       " ('이/Determiner', 0.006753643412893798),\n",
       " ('정말/Noun', 0.006332363511894575),\n",
       " ('그리고/Conjunction', 0.006016403586145158),\n",
       " ('./Punctuation', 0.005937413604707803),\n",
       " ('이렇다/Adjective', 0.005687278663489514),\n",
       " ('하지만/Conjunction', 0.004871048855303519),\n",
       " ('보다/Verb', 0.004489263945022973)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_most_common(\".\", 10, \"Punctuation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "school_cell_uuid": "d90d43ba0cfd45e18bbe9242fcbff97e"
   },
   "outputs": [],
   "source": [
    "def korean_bigram_prob(c, w):\n",
    "    context = tokenize(c)[1]\n",
    "    word = tokenize(w)[1]\n",
    "    return bigram[context][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "school_cell_uuid": "5e8abc4db7fc48aba42bd019d321694f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34717143471714346"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_bigram_prob(\"이\", \"영화\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "school_cell_uuid": "9583a21ec46843c194948ee0c3f80945"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00024180648041367508"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_bigram_prob(\"영화\", \"이\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "school_cell_uuid": "d9eefcc2bf514f459cecb4ec175a12ac"
   },
   "outputs": [],
   "source": [
    "def korean_generate_sentence(seed=None, debug=False):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    c = \"SS\"\n",
    "    sentence = []\n",
    "    while True:\n",
    "        if c not in bigram:\n",
    "            break\n",
    "        words, probs = zip(*[(k, v) for k, v in bigram[c].items()])\n",
    "        idx = np.argmax(np.random.multinomial(1, probs, (1,)))\n",
    "        w = words[idx]\n",
    "        \n",
    "        if w == \"SE\":\n",
    "            break            \n",
    "        \n",
    "        w2 = w.split(\"/\")[0]\n",
    "        pos = w.split(\"/\")[1]\n",
    "        \n",
    "        if c == \"SS\":\n",
    "            sentence.append(w2.title())\n",
    "        elif c in [\"`\", \"\\\"\", \"'\", \"(\"]:\n",
    "            sentence.append(w2)\n",
    "        elif w2 in [\"'\", \".\", \",\", \")\", \":\", \";\", \"?\"]:\n",
    "            sentence.append(w2)\n",
    "        elif pos in [\"Josa\", \"Punctuation\", \"Suffix\"]:\n",
    "            sentence.append(w2)\n",
    "        elif w in [\"임/Noun\", \"것/Noun\", \"는걸/Noun\", \"릴때/Noun\",\n",
    "                   \"되다/Verb\", \"이다/Verb\", \"하다/Verb\", \"이다/Adjective\"]:\n",
    "            sentence.append(w2)\n",
    "        else:\n",
    "            sentence.append(\" \" + w2)\n",
    "        c = w\n",
    "        \n",
    "        if debug:\n",
    "            print(w)\n",
    "            \n",
    "    return \"\".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "school_cell_uuid": "ddc224d6918c49cfbc6d5608ad92564b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'김재중 연기 너무 부족하다.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_generate_sentence(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "school_cell_uuid": "e6ebd75c8e10451cb51fdb7ae3d6aa37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'비디오 값을 당한 졸작...;;'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_generate_sentence(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "school_cell_uuid": "6d30e0824acd4ac18ffba3512d08db4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'다 잘되다 바랬는데 ㅠㅠ 나빳음 ㅜㅜ 말에 의 노래가 받다 순신와 코드랑은 너무 어이가 안 보이다 야하다 솔직하다 난 결국 영화장르는 짐작. 악역 등장 시키다 비겁 한시도 아니다. 해외 영화 다'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_generate_sentence(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "school_cell_uuid": "0a08313b5715418595343f42e331e42c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'재미도 개 꿀잼'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_generate_sentence(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "school_cell_uuid": "fd497eb0d0c1445b839dd038c09b133f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이영화를 봣는데... 가치도 마찬가지 아니다 사담에 이렇다 상황 설정을하다 잖다 유머로 소름 끼가 맞다 재미 없다 이 영화를 너무 웃기다 컨셉을 바탕이라곤 해도 과언이 이렇다 막장해 주기에도 긴장감이라고는 생각 쓰다 ㅋ 키 센빠이 없다 괴상하고 드라마가 이것 같다 영화 수준의 관계를 자다 들리다 박세영 애교 부리는 범인 잡다 사람들 도덕 차리다 그것이 원하다 배우가 대중화되다 많다 때 ㅠ'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_generate_sentence(46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "974f69b35d204567af08c3d91434b4d6"
   },
   "source": [
    "## 확률론적 언어 모형의 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "dd8625a8f9fc451684f0595c2a6564c8"
   },
   "source": [
    "확률론적 언어 모형은 다음과 같은 분야에 광범위하게 활용할 수 있다.\n",
    "\n",
    "* 철자 및 문법 교정(Spell Correction)\n",
    "* 음성 인식(Speech Recognition)\n",
    "* 자동 번역(Machine Translation)\n",
    "* 자동 요약(Summarization)\n",
    "* 챗봇(Question-Answering)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}